{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pmnmxUE0YBr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sx8IYjw10ccx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMK_Br5S0cfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.fahdmirza.com/2024/09/how-to-implement-agentic-rag-with-llama.html"
      ],
      "metadata": {
        "id": "-SqlqYyI1cA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install git+https://github.com/huggingface/accelerate\n",
        "!pip install huggingface_hub\n",
        "!pip install sentencepiece\n",
        "!pip install bitsandbytes\n",
        "\n",
        "\n",
        "!pip install haystack-ai duckduckgo-api-haystack transformers sentence-transformers datasets\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wM_0pb9s0ciu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال راج وشات مع داتا سيت وان لم يجد يبحث  ف الانترنت"
      ],
      "metadata": {
        "id": "0pJq3QEm-x6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################################################################################################################"
      ],
      "metadata": {
        "id": "Z_ms5rcP-vwy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfyNJv08-6x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
        "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
        "\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "doc_embedder.warm_up()\n",
        "\n",
        "docs_with_embeddings = doc_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "RRWDARJp0eFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n"
      ],
      "metadata": {
        "id": "xMqg-X5F2anH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from haystack.components.generators import HuggingFaceLocalGenerator\n",
        "\n",
        "generator = HuggingFaceLocalGenerator(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    huggingface_pipeline_kwargs={\"device_map\":\"auto\",\n",
        "                                 \"torch_dtype\":torch.bfloat16},\n",
        "    generation_kwargs={\"max_new_tokens\": 22})\n",
        "\n",
        "generator.warm_up()\n",
        "\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "  What is the capital of Australia?<|eot_id|>\n",
        "  <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "generator.run(prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "AFA_xKrW0mV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retriever = InMemoryEmbeddingRetriever(document_store, top_k=1)"
      ],
      "metadata": {
        "id": "6X9TFmpa0qOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Answer the following query given the documents.\n",
        "If the answer is not contained within the documents reply with 'no_answer'.\n",
        "If the answer is contained within the documents, start the answer with \"FROM THE KNOWLEDGE BASE: \".\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=prompt_template)"
      ],
      "metadata": {
        "id": "BPIkkBwC0vNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.routers import ConditionalRouter\n",
        "\n",
        "routes = [\n",
        "    {\n",
        "        \"condition\": \"{{'no_answer' in replies[0]}}\",\n",
        "        \"output\": \"{{query}}\",\n",
        "        \"output_name\": \"go_to_websearch\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    {\n",
        "        \"condition\": \"{{'no_answer' not in replies[0]}}\",\n",
        "        \"output\": \"{{replies[0]}}\",\n",
        "        \"output_name\": \"answer\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    ]\n",
        "\n",
        "router = ConditionalRouter(routes)"
      ],
      "metadata": {
        "id": "olUT5Bbu0122"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_api_haystack import DuckduckgoApiWebSearch\n",
        "\n",
        "websearch = DuckduckgoApiWebSearch(top_k=5)\n",
        "\n",
        "prompt_template_after_websearch = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Answer the following query given the documents retrieved from the web.\n",
        "Start the answer with \"FROM THE WEB: \".\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder_after_websearch = PromptBuilder(template=prompt_template_after_websearch)\n",
        "\n"
      ],
      "metadata": {
        "id": "DCERGM-D1Jce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.joiners import BranchJoiner\n",
        "\n",
        "prompt_joiner  = BranchJoiner(str)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xXnDa9tO1K-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"text_embedder\", text_embedder)\n",
        "pipe.add_component(\"retriever\", retriever)\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"prompt_joiner\", prompt_joiner)\n",
        "pipe.add_component(\"llm\", generator)\n",
        "pipe.add_component(\"router\", router)\n",
        "pipe.add_component(\"websearch\", websearch)\n",
        "pipe.add_component(\"prompt_builder_after_websearch\", prompt_builder_after_websearch)\n",
        "\n",
        "pipe.connect(\"text_embedder\", \"retriever\")\n",
        "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder\", \"prompt_joiner\")\n",
        "pipe.connect(\"prompt_joiner\", \"llm\")\n",
        "pipe.connect(\"llm.replies\", \"router.replies\")\n",
        "pipe.connect(\"router.go_to_websearch\", \"websearch.query\")\n",
        "pipe.connect(\"router.go_to_websearch\", \"prompt_builder_after_websearch.query\")\n",
        "pipe.connect(\"websearch.documents\", \"prompt_builder_after_websearch.documents\")\n",
        "pipe.connect(\"prompt_builder_after_websearch\", \"prompt_joiner\")"
      ],
      "metadata": {
        "id": "qfq1IHqg1SxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(query):\n",
        "  result = pipe.run({\"text_embedder\": {\"text\": query}, \"prompt_builder\": {\"query\": query}, \"router\": {\"query\": query}})\n",
        "  print(result[\"router\"][\"answer\"])"
      ],
      "metadata": {
        "id": "xng95QTC1WjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why did people build Great Pyramid of Giza?\"\n",
        "\n",
        "get_answer(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxWKzJ2K1ZWG",
        "outputId": "de681991-016f-4dc4-cc19-c7c66599ede6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/duckduckgo_api_haystack/duckduckgoapi.py:176: UserWarning: 'api' backend is deprecated, using backend='auto'\n",
            "  results = self.ddgs.text(**payload)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "FROM THE WEB: According to the retrieved documents, people built the Great Pyramid of Giza as a tomb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################################"
      ],
      "metadata": {
        "id": "T9IjqXIH-srC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mTaCxKaZ4H3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpKLDYJ64H0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKgHLXwG4Hw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fp3JVjHG4Htn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoRbS7Uc4Hqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6e6AkpG04HlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFg3E7f24Ijj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gw9ih7Go4Ijj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0liV_8E4Ijj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.fahdmirza.com/2024/09/how-to-implement-agentic-rag-with-llama.html"
      ],
      "metadata": {
        "id": "_WASc90d4Ijk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install git+https://github.com/huggingface/accelerate\n",
        "!pip install huggingface_hub\n",
        "!pip install sentencepiece\n",
        "!pip install bitsandbytes\n",
        "\n",
        "\n",
        "!pip install haystack-ai duckduckgo-api-haystack transformers sentence-transformers datasets\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NV5rq9VA4Ijk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
        "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
        "\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "doc_embedder.warm_up()\n",
        "\n",
        "docs_with_embeddings = doc_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "i-t_JaJv4Ijl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n"
      ],
      "metadata": {
        "id": "gI0MFWes4Ijm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from haystack.components.generators import HuggingFaceLocalGenerator\n",
        "\n",
        "generator = HuggingFaceLocalGenerator(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    huggingface_pipeline_kwargs={\"device_map\":\"auto\",\n",
        "                                 \"torch_dtype\":torch.bfloat16},\n",
        "    generation_kwargs={\"max_new_tokens\": 22})\n",
        "\n",
        "generator.warm_up()\n",
        "\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "  What is the capital of Australia?<|eot_id|>\n",
        "  <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "generator.run(prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "OpH8dcxN4Ijm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retriever = InMemoryEmbeddingRetriever(document_store, top_k=1)"
      ],
      "metadata": {
        "id": "IRmR2INl4Ijn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Answer the following query given the documents.\n",
        "If the answer is not contained within the documents reply with 'no_answer'.\n",
        "If the answer is contained within the documents, start the answer with \"FROM THE KNOWLEDGE BASE: \".\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=prompt_template)"
      ],
      "metadata": {
        "id": "RqTWwrQb4Ijn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.routers import ConditionalRouter\n",
        "\n",
        "routes = [\n",
        "    {\n",
        "        \"condition\": \"{{'no_answer' in replies[0]}}\",\n",
        "        \"output\": \"{{query}}\",\n",
        "        \"output_name\": \"go_to_websearch\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    {\n",
        "        \"condition\": \"{{'no_answer' not in replies[0]}}\",\n",
        "        \"output\": \"{{replies[0]}}\",\n",
        "        \"output_name\": \"answer\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    ]\n",
        "\n",
        "router = ConditionalRouter(routes)"
      ],
      "metadata": {
        "id": "4527xDFm4Ijo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_api_haystack import DuckduckgoApiWebSearch\n",
        "\n",
        "websearch = DuckduckgoApiWebSearch(top_k=5)\n",
        "\n",
        "prompt_template_after_websearch = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Answer the following query given the documents retrieved from the web.\n",
        "Start the answer with \"FROM THE WEB: \".\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder_after_websearch = PromptBuilder(template=prompt_template_after_websearch)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xu13Spms4Ijo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.joiners import BranchJoiner\n",
        "\n",
        "prompt_joiner  = BranchJoiner(str)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGlYGf9g4Ijp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"text_embedder\", text_embedder)\n",
        "pipe.add_component(\"retriever\", retriever)\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"prompt_joiner\", prompt_joiner)\n",
        "pipe.add_component(\"llm\", generator)\n",
        "pipe.add_component(\"router\", router)\n",
        "pipe.add_component(\"websearch\", websearch)\n",
        "pipe.add_component(\"prompt_builder_after_websearch\", prompt_builder_after_websearch)\n",
        "\n",
        "pipe.connect(\"text_embedder\", \"retriever\")\n",
        "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder\", \"prompt_joiner\")\n",
        "pipe.connect(\"prompt_joiner\", \"llm\")\n",
        "pipe.connect(\"llm.replies\", \"router.replies\")\n",
        "pipe.connect(\"router.go_to_websearch\", \"websearch.query\")\n",
        "pipe.connect(\"router.go_to_websearch\", \"prompt_builder_after_websearch.query\")\n",
        "pipe.connect(\"websearch.documents\", \"prompt_builder_after_websearch.documents\")\n",
        "pipe.connect(\"prompt_builder_after_websearch\", \"prompt_joiner\")"
      ],
      "metadata": {
        "id": "WWJ-uDi14Ijp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(query):\n",
        "  result = pipe.run({\"text_embedder\": {\"text\": query}, \"prompt_builder\": {\"query\": query}, \"router\": {\"query\": query}})\n",
        "  print(result[\"router\"][\"answer\"])"
      ],
      "metadata": {
        "id": "cIvj1sf14Ijq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why did people build Great Pyramid of Giza?\"\n",
        "\n",
        "get_answer(query)"
      ],
      "metadata": {
        "id": "qU4xA1BJ4Ijq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9MBNwxAM4sAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdIBAy6a4r9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dmt40X0b4r6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQcIG4fl4r3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DcNP8oFm4rz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
        "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
        "\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "doc_embedder.warm_up()\n",
        "\n",
        "docs_with_embeddings = doc_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
        "\n",
        "\n",
        "import torch\n",
        "from haystack.components.generators import HuggingFaceLocalGenerator\n",
        "\n",
        "generator = HuggingFaceLocalGenerator(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    huggingface_pipeline_kwargs={\"device_map\":\"auto\",\n",
        "                                 \"torch_dtype\":torch.bfloat16},\n",
        "    generation_kwargs={\"max_new_tokens\": 22})\n",
        "\n",
        "generator.warm_up()\n",
        "\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "  What is the capital of Australia?<|eot_id|>\n",
        "  <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "generator.run(prompt)\n",
        "\n",
        "\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retriever = InMemoryEmbeddingRetriever(document_store, top_k=1)\n",
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Answer the following query given the documents.\n",
        "If the answer is not contained within the documents reply with 'no_answer'.\n",
        "If the answer is contained within the documents, start the answer with \"FROM THE KNOWLEDGE BASE: \".\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=prompt_template)\n",
        "from haystack.components.routers import ConditionalRouter\n",
        "\n",
        "routes = [\n",
        "    {\n",
        "        \"condition\": \"{{'no_answer' in replies[0]}}\",\n",
        "        \"output\": \"{{query}}\",\n",
        "        \"output_name\": \"go_to_websearch\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    {\n",
        "        \"condition\": \"{{'no_answer' not in replies[0]}}\",\n",
        "        \"output\": \"{{replies[0]}}\",\n",
        "        \"output_name\": \"answer\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    ]\n",
        "\n",
        "router = ConditionalRouter(routes)\n",
        "from duckduckgo_api_haystack import DuckduckgoApiWebSearch\n",
        "\n",
        "websearch = DuckduckgoApiWebSearch(top_k=5)\n",
        "\n",
        "prompt_template_after_websearch = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Answer the following query given the documents retrieved from the web.\n",
        "Start the answer with \"FROM THE WEB: \".\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder_after_websearch = PromptBuilder(template=prompt_template_after_websearch)\n",
        "\n",
        "\n",
        "from haystack.components.joiners import BranchJoiner\n",
        "\n",
        "prompt_joiner  = BranchJoiner(str)\n",
        "\n",
        "\n",
        "\n",
        "from haystack import Pipeline\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"text_embedder\", text_embedder)\n",
        "pipe.add_component(\"retriever\", retriever)\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"prompt_joiner\", prompt_joiner)\n",
        "pipe.add_component(\"llm\", generator)\n",
        "pipe.add_component(\"router\", router)\n",
        "pipe.add_component(\"websearch\", websearch)\n",
        "pipe.add_component(\"prompt_builder_after_websearch\", prompt_builder_after_websearch)\n",
        "\n",
        "pipe.connect(\"text_embedder\", \"retriever\")\n",
        "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder\", \"prompt_joiner\")\n",
        "pipe.connect(\"prompt_joiner\", \"llm\")\n",
        "pipe.connect(\"llm.replies\", \"router.replies\")\n",
        "pipe.connect(\"router.go_to_websearch\", \"websearch.query\")\n",
        "pipe.connect(\"router.go_to_websearch\", \"prompt_builder_after_websearch.query\")\n",
        "pipe.connect(\"websearch.documents\", \"prompt_builder_after_websearch.documents\")\n",
        "pipe.connect(\"prompt_builder_after_websearch\", \"prompt_joiner\")\n",
        "def get_answer(query):\n",
        "  result = pipe.run({\"text_embedder\": {\"text\": query}, \"prompt_builder\": {\"query\": query}, \"router\": {\"query\": query}})\n",
        "  print(result[\"router\"][\"answer\"])\n",
        "query = \"Why did people build Great Pyramid of Giza?\"\n",
        "\n",
        "get_answer(query)"
      ],
      "metadata": {
        "id": "fIo_gzzt4sgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJSUzI055DTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcUWob8q5DRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBmS6Xgq5DOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRSPPBeB5DK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document, Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "import torch\n",
        "from haystack.components.generators import HuggingFaceLocalGenerator\n",
        "\n",
        "# إعداد مخزن الوثائق وتحميل البيانات\n",
        "document_store = InMemoryDocumentStore()\n",
        "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
        "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
        "\n",
        "# إعداد embedder للوثائق\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "doc_embedder.warm_up()\n",
        "\n",
        "# إضافة embeddings للوثائق وحفظها في المخزن\n",
        "docs_with_embeddings = doc_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
        "\n",
        "# إعداد نموذج التوليد\n",
        "generator = HuggingFaceLocalGenerator(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    huggingface_pipeline_kwargs={\n",
        "        \"device_map\": \"auto\",\n",
        "        \"torch_dtype\": torch.bfloat16\n",
        "    },\n",
        "    generation_kwargs={\"max_new_tokens\": 20}  # زيادة عدد التوكنز للحصول على إجابات أطول\n",
        ")\n",
        "generator.warm_up()\n",
        "\n",
        "# إعداد مكونات الـ Pipeline\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retriever = InMemoryEmbeddingRetriever(document_store, top_k=1)  # زيادة top_k لاسترجاع المزيد من الوثائق ذات الصلة\n",
        "\n",
        "# تعديل قالب الـ prompt ليكون أكثر وضوحاً\n",
        "prompt_template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Please answer the following question using only the information provided in the documentation.\n",
        "If the answer is not in the documentation, respond with 'Information not available in the database'.\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "{{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{query}}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=prompt_template)\n",
        "\n",
        "# إنشاء وتكوين الـ Pipeline\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"text_embedder\", text_embedder)\n",
        "pipe.add_component(\"retriever\", retriever)\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"llm\", generator)\n",
        "\n",
        "# ربط مكونات الـ Pipeline\n",
        "pipe.connect(\"text_embedder\", \"retriever\")\n",
        "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder\", \"llm\")\n",
        "\n",
        "# دالة للحصول على الإجابة\n",
        "def get_answer(query):\n",
        "    result = pipe.run({\"text_embedder\": {\"text\": query}, \"prompt_builder\": {\"query\": query}})\n",
        "    return result[\"llm\"][\"replies\"][0]\n",
        "\n",
        "# مثال على الاستخدام\n",
        "query = \"In what year did an Arab force led by the Muslim general Muawiyah I invade Rhodes?\"\n",
        "answer = get_answer(query)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "YkKj6Ayv5DHo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}